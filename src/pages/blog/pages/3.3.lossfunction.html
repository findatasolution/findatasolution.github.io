<h1>Loss Functions of Popular Machine Learning Algorithms</h1>
<h2>Master table</h2>
  <table>
    <thead>
      <tr>
        <th></th> 
        <th>Algorithm</th>
        <th>Typical Use Case</th>
        <th>Common Loss Function(s)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="assets\images\LR.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Linear Regression</td>
        <td>Regression</td>
        <td>MSE, MAEs</td>
      </tr>
      <tr>
        <td><img src="assets\images\LogR.png" style="width: 100px; height: 100px; object-fit: contain;"></td>
        <td>Logistic Regression</td>
        <td>Binary Classification</td>
        <td>Log Loss</td>
      </tr>
      <tr>
        <td><img src="assets\images\DecisionTree.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Decision Tree:
          <br> 1.1 Bagging - Random Forest
          <br> 1.2.1 Boosting - AdaBoost
          <br> 1.2.2 Boosting - Gradient Boosting
          <br> 1.2.2.1 Boosting - Gradient Boosting - CatBoost
          <br> 1.2.2.2 Boosting - Gradient Boosting - LightGBM
          <br> 1.2.2.3 Boosting - Gradient Boosting - XGBoost
        </td>
        <td>Classification <br> Regression</td>
        <td>Gini Impurity,Entropy (Information Gain) <br> MSE, MAE, Huber Loss </td>
      </tr>
      <tr>
        <td></td>
        <td>SVM</td>
        <td>Classification</td>
        <td>Hinge Loss, Squared Hinge Loss, Logistic Loss</td>
      </tr>
      <tr>
        <td></td>
        <td>k-NN</td>
        <td>Classification, <br> Regression</td>
        <td>No direct loss; uses distance metrics (e.g., Euclidean, Manhattan)</td>
      </tr>
      <tr>
        <td></td>
        <td>Naive Bayes</td>
        <td>Classification</td>
        <td>Negative Log-Likelihood, <br>no explicit loss function</td>
      </tr>
      <tr>
        <td></td>
        <td>Neural Networks</td>
        <td>Classification, <br>Regression</td>
        <td>Regression: MSE, MAE <br>Classification: Cross-Entropy / Log loss</td>
      </tr>
      <tr>
        <td></td>
        <td>K-Means</td>
        <td>Unsupervised</td>
        <td>Within-Cluster Sum of Squares (WCSS)</td>
      </tr>
      <tr>
        <td></td>
        <td>Reinforcement Learning</td>
        <td>Decision Making</td>
        <td>TD Error, Policy Gradient Loss, Actor-Critic Loss</td>
      </tr>
    </tbody>
  </table>

  <div style="text-align: center;">
    <img src="/../assets/images/lossfunctions.png" alt="Common Loss Functions" style="max-width: 100%; height: auto; margin: 20px 0;">
    <p style="font-style: italic; color: #666;">Figure: Visual comparison of common loss functions showing their behavior and characteristics</p>
  </div>
  <h2>MSE - Mean Squared Error</h2>
  <p>MSE is a loss function that measures the average of the squares of the errors. It is a popular loss function for regression problems.</p>
  <p>The MSE function is given by:</p>
  <p>
      $$\operatorname{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left( Y_{i} - \hat{Y_{i}} \right)^{2}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>MSE is sensitive to outliers because it squares the errors.</p>
  
  <h2>MAE - Mean Absolute Error</h2>
  <p>MAE is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The MAE function is given by:</p>
  <p>
      $$\operatorname{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| Y_{i} - \hat{Y_{i}} \right|$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>MAE is less sensitive to outliers because it does not square the errors.</p>
  <h2>R square</h2>
  <p>R square is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The R square function is given by:</p>
  <p>
      $$R^2 = 1 - \frac{\sum_{i=1}^{n} \left( Y_{i} - \hat{Y_{i}} \right)^{2}}{\sum_{i=1}^{n} \left( Y_{i} - \bar{Y} \right)^{2}}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>Value range from 0 to 1, 1 is the best fit.</p>
  <p>Common thresholds are: 
    - > 0.8 is good
    - 0.5 - 0.8 is fair
    - = 0.5 random fit.</p>
  <p>R square is used to measure the goodness of fit of a regression model.</p>

  <h2>Log Loss - Binary Cross-Entropy</h2>
  <p>Log Loss is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The Log Loss function is given by:</p>
  <p>
      $$L(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>Value range from 0 to infinity, 0 is the best fit.</p>
  <p>Common thresholds are:
    - < 0.2 : Good
    - 0.2 - 0.5 : Fair
    - > 0.5: Bad
  </p>
  <p>Log Loss is used in binary classification problems.</p>

  <h1>Learning process</h1>
  <p>Boosting Model learning between node, improve the model by loss function. This example work on Logistic Loss</p> 
  <p>The loss function is given by:</p>
  <p>
      $$L(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>Log Loss is used in binary classification problems.</p>
  <p>The loss function is used to improve the model by minimizing the loss function.</p>
  <p> </p>
  
