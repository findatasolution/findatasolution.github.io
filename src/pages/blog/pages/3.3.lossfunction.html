<h1>Loss Functions of Popular Machine Learning Algorithms</h1>
<h2>Master table</h2>
  <table>
    <thead>
      <tr>
        <th></th> 
        <th>Algorithm</th>
        <th>Typical Use Case</th>
        <th>Common Loss Function(s)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="assets\images\LR.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Linear Regression</td>
        <td>Regression</td>
        <td>MSE, MAEs</td>
      </tr>
      <tr>
        <td><img src="assets\images\LogR.png" style="width: 100px; height: 100px; object-fit: contain;"></td>
        <td>Logistic Regression</td>
        <td>Binary Classification</td>
        <td>Log Loss</td>
      </tr>
      <tr>
        <td><img src="assets\images\DecisionTree.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Decision Tree:
          <br> 1.1 Bagging - Random Forest
          <br> 1.2.1 Boosting - AdaBoost
          <br> 1.2.2 Boosting - Gradient Boosting
          <br> 1.2.2.1 Boosting - Gradient Boosting - CatBoost
          <br> 1.2.2.2 Boosting - Gradient Boosting - LightGBM
          <br> 1.2.2.3 Boosting - Gradient Boosting - XGBoost
        </td>
        <td>Classification <br> Regression</td>
        <td>Gini Impurity,Entropy (Information Gain) <br> MSE, MAE, Huber Loss </td>
      </tr>
      <tr>
        <td></td>
        <td>SVM</td>
        <td>Classification</td>
        <td>Hinge Loss, Squared Hinge Loss, Logistic Loss</td>
      </tr>
      <tr>
        <td></td>
        <td>k-NN</td>
        <td>Classification, <br> Regression</td>
        <td>No direct loss; uses distance metrics (e.g., Euclidean, Manhattan)</td>
      </tr>
      <tr>
        <td></td>
        <td>Naive Bayes</td>
        <td>Classification</td>
        <td>Negative Log-Likelihood, <br>no explicit loss function</td>
      </tr>
      <tr>
        <td></td>
        <td>Neural Networks</td>
        <td>Classification, <br>Regression</td>
        <td>Regression: MSE, MAE <br>Classification: Cross-Entropy / Log loss</td>
      </tr>
      <tr>
        <td></td>
        <td>K-Means</td>
        <td>Unsupervised</td>
        <td>Within-Cluster Sum of Squares (WCSS)</td>
      </tr>
      <tr>
        <td></td>
        <td>Reinforcement Learning</td>
        <td>Decision Making</td>
        <td>TD Error, Policy Gradient Loss, Actor-Critic Loss</td>
      </tr>
    </tbody>
  </table>

  <h2>MSE - Mean Squared Error</h2>
  <p>MSE is a loss function that measures the average of the squares of the errors. It is a popular loss function for regression problems.</p>
  <p>The MSE function is given by:</p>
  <p>
      $$\operatorname{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left( Y_{i} - \hat{Y_{i}} \right)^{2}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>MSE is sensitive to outliers because it squares the errors.</p>
  <div style="text-align: center;">
    <img src="assets/images/lossfunctions.png" alt="Common Loss Functions" style="max-width: 80%; height: auto; margin: 20px 0;">
    <p style="font-style: italic; color: #666;"></p>
  </div>

  <h2>MAE - Mean Absolute Error</h2>
  <p>MAE is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The MAE function is given by:</p>
  <p>
      $$\operatorname{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| Y_{i} - \hat{Y_{i}} \right|$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>MAE is less sensitive to outliers because it does not square the errors.</p>
  <h2>R square</h2>
  <p>R square is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The R square function is given by:</p>
  <p>
      $$R^2 = 1 - \frac{\sum_{i=1}^{n} \left( Y_{i} - \hat{Y_{i}} \right)^{2}}{\sum_{i=1}^{n} \left( Y_{i} - \bar{Y} \right)^{2}}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>Value range from 0 to 1, 1 is the best fit.</p>
  <p>Common thresholds are:</p>
  <p>- Greater than 0.8 is considered good</p>
  <p>- Between 0.5 and 0.8 is considered fair</p>
  <p>- Equal to 0.5 indicates random fit</p>
  <p>R square is used to measure the goodness of fit of a regression model.</p>
  <div style="text-align: center;">
    <img src="assets/images/rsquare.png" alt="Common Loss Functions" style="max-width: 80%; height: auto; margin: 20px 0;">
    <p style="font-style: italic; color: #666;"></p>
  </div>
  <h2>Log Loss - Binary Cross-Entropy</h2>
  <p>Log Loss is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The Log Loss function is given by:</p>
  <p>
      $$L(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>Value range from 0 to infinity, 0 is the best fit.</p>
  <p>Common thresholds are:
    - < 0.2 : Good
    - 0.2 - 0.5 : Fair
    - > 0.5: Bad
  </p>
  <p>Log Loss is used in binary classification problems.</p>

  <h2>Log Loss - Binary Cross-Entropy</h2>

<p><strong>Log Loss</strong> (hay còn gọi là <em>Logarithmic Loss</em> hoặc <em>Cross-Entropy Loss</em>) là một hàm đánh giá sai số giữa xác suất dự đoán và nhãn thực tế trong bài toán phân loại nhị phân.</p>

<h3>Công thức:</h3>
<p>Với một mẫu có nhãn thực tế <code>y ∈ {0, 1}</code> và xác suất dự đoán là <code>ŷ ∈ (0, 1)</code>, công thức Log Loss như sau:</p>

<pre><code>LogLoss = - (y * log(ŷ) + (1 - y) * log(1 - ŷ))</code></pre>

<p>Với N mẫu:</p>
<pre><code>LogLoss_avg = - (1/N) * Σ [y<sub>i</sub> * log(ŷ<sub>i</sub>) + (1 - y<sub>i</sub>) * log(1 - ŷ<sub>i</sub>)]</code></pre>

<h3>Ý nghĩa:</h3>
<ul>
  <li>Nếu dự đoán xác suất gần đúng với thực tế (ví dụ: dự đoán 0.99 cho nhãn là 1) → Log Loss nhỏ → mô hình tốt.</li>
  <li>Nếu dự đoán sai với xác suất cao (ví dụ: 0.01 cho nhãn là 1) → Log Loss lớn → bị phạt nặng.</li>
  <li>Log Loss luôn &ge; 0. Giá trị càng nhỏ càng tốt.</li>
</ul>

<h3>Ví dụ:</h3>
<table border="1" cellpadding="6">
  <thead>
    <tr>
      <th>Y (Nhãn thật)</th>
      <th>Ŷ (Xác suất dự đoán)</th>
      <th>Log Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.9</td>
      <td>-log(0.9) ≈ 0.105</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.1</td>
      <td>-log(0.1) ≈ 2.302</td>
    </tr>
    <tr>
      <td>0</td>
      <td>0.9</td>
      <td>-log(1 - 0.9) = -log(0.1) ≈ 2.302</td>
    </tr>
    <tr>
      <td>0</td>
      <td>0.1</td>
      <td>-log(0.9) ≈ 0.105</td>
    </tr>
  </tbody>
</table>

<h3>Ứng dụng:</h3>
<ul>
  <li>Được dùng phổ biến trong các bài toán phân loại nhị phân.</li>
  <li>Là hàm loss mặc định trong nhiều thư viện như <code>sklearn.metrics.log_loss</code>, <code>binary_crossentropy</code> trong Keras,...</li>
  <li>Đặc biệt hiệu quả với các mô hình dự đoán xác suất như Logistic Regression, Neural Networks.</li>
</ul>

<p>Nói ngắn gọn, <strong>Log Loss phạt nặng các dự đoán sai với độ tin cậy cao</strong>, do đó nó là một thước đo rất hữu ích để đánh giá mô hình phân loại.</p>

