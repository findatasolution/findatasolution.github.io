<h1>Loss Functions of Popular Machine Learning Algorithms</h1>
<h2>Master table</h2>
  <table>
    <thead>
      <tr>
        <th></th> 
        <th>Algorithm</th>
        <th>Typical Use Case</th>
        <th>Common Loss Function(s)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="assets\images\LR.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Linear Regression</td>
        <td>Regression</td>
        <td>MSE, MAEs</td>
      </tr>
      <tr>
        <td><img src="assets\images\LogR.png" style="width: 100px; height: 100px; object-fit: contain;"></td>
        <td>Logistic Regression</td>
        <td>Binary Classification</td>
        <td>Log Loss</td>
      </tr>
      <tr>
        <td><img src="assets\images\DecisionTree.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Decision Tree:
          <br> 1.1 Bagging - Random Forest
          <br> 1.2.1 Boosting - AdaBoost
          <br> 1.2.2 Boosting - Gradient Boosting
          <br> 1.2.2.1 Boosting - Gradient Boosting - CatBoost
          <br> 1.2.2.2 Boosting - Gradient Boosting - LightGBM
          <br> 1.2.2.3 Boosting - Gradient Boosting - XGBoost
        </td>
        <td>Classification <br> Regression</td>
        <td>Gini Impurity,Entropy (Information Gain) <br> MSE, MAE, Huber Loss </td>
      </tr>
      <tr>
        <td></td>
        <td>SVM</td>
        <td>Classification</td>
        <td>Hinge Loss, Squared Hinge Loss, Logistic Loss</td>
      </tr>
      <tr>
        <td></td>
        <td>k-NN</td>
        <td>Classification, <br> Regression</td>
        <td>No direct loss; uses distance metrics (e.g., Euclidean, Manhattan)</td>
      </tr>
      <tr>
        <td></td>
        <td>Naive Bayes</td>
        <td>Classification</td>
        <td>Negative Log-Likelihood, <br>no explicit loss function</td>
      </tr>
      <tr>
        <td></td>
        <td>Neural Networks</td>
        <td>Classification, <br>Regression</td>
        <td>Regression: MSE, MAE <br>Classification: Cross-Entropy / Log loss</td>
      </tr>
      <tr>
        <td></td>
        <td>K-Means</td>
        <td>Unsupervised</td>
        <td>Within-Cluster Sum of Squares (WCSS)</td>
      </tr>
      <tr>
        <td></td>
        <td>Reinforcement Learning</td>
        <td>Decision Making</td>
        <td>TD Error, Policy Gradient Loss, Actor-Critic Loss</td>
      </tr>
    </tbody>
  </table>

  <h2>MSE - Mean Squared Error</h2>
  <p>MSE is a loss function that measures the average of the squares of the errors. It is a popular loss function for regression problems.</p>
  <p>The MSE function is given by:</p>
  <p>
      $$\operatorname{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left( Y_{i} - \hat{Y_{i}} \right)^{2}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>MSE is sensitive to outliers because it squares the errors.</p>
  <div style="text-align: center;">
    <img src="assets/images/lossfunctions.png" alt="Common Loss Functions" style="max-width: 80%; height: auto; margin: 20px 0;">
    <p style="font-style: italic; color: #666;"></p>
  </div>

  <h2>MAE - Mean Absolute Error</h2>
  <p>MAE is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The MAE function is given by:</p>
  <p>
      $$\operatorname{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| Y_{i} - \hat{Y_{i}} \right|$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>MAE is less sensitive to outliers because it does not square the errors.</p>
  <h2>R square</h2>
  <p>R square is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The R square function is given by:</p>
  <p>
      $$R^2 = 1 - \frac{\sum_{i=1}^{n} \left( Y_{i} - \hat{Y_{i}} \right)^{2}}{\sum_{i=1}^{n} \left( Y_{i} - \bar{Y} \right)^{2}}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>Value range from 0 to 1, 1 is the best fit.</p>
  <p>Common thresholds are:</p>
  <p>- Greater than 0.8 is considered good</p>
  <p>- Between 0.5 and 0.8 is considered fair</p>
  <p>- Equal to 0.5 indicates random fit</p>
  <p>R square is used to measure the goodness of fit of a regression model.</p>
  <div style="text-align: center;">
    <img src="assets/images/rsquare.png" alt="Common Loss Functions" style="max-width: 80%; height: auto; margin: 20px 0;">
    <p style="font-style: italic; color: #666;"></p>
  </div>
  <h2>Log Loss</h2>

  <p><strong>Log Loss</strong> (also known as <em>Logarithmic Loss</em> or <em>Cross-Entropy Loss</em>) is a loss function that measures the error between predicted probabilities and actual class labels in binary classification problems.</p>

  <h3>Formula:</h3>
  <p>Given a true label <code>y ∈ {0, 1}</code> and a predicted probability <code>ŷ ∈ (0, 1)</code>, the log loss for a single observation is defined as:</p>

  <pre><code>LogLoss = - (y * log(ŷ) + (1 - y) * log(1 - ŷ))</code></pre>

  <p>For N observations:</p>
  <p>
    $$L(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>

  <h3>Meaning:</h3>
  <ul>
    <li>If the predicted probability is close to the true label (e.g., predicting 0.99 for a true label of 1) → small log loss → good prediction.</li>
    <li>If the model is confident but wrong (e.g., predicting 0.01 for a true label of 1) → large log loss → heavily penalized.</li>
    <li>Log loss is always ≥ 0. The lower the value, the better the model.</li>
  </ul>

  <h3>Example:</h3>
  <table border="1" cellpadding="6">
    <thead>
      <tr>
        <th>True Label (y)</th>
        <th>Predicted Probability (ŷ)</th>
        <th>Log Loss</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1</td>
        <td>0.9</td>
        <td>-log(0.9) ≈ 0.105</td>
      </tr>
      <tr>
        <td>1</td>
        <td>0.1</td>
        <td>-log(0.1) ≈ 2.302</td>
      </tr>
      <tr>
        <td>0</td>
        <td>0.9</td>
        <td>-log(1 - 0.9) = -log(0.1) ≈ 2.302</td>
      </tr>
      <tr>
        <td>0</td>
        <td>0.1</td>
        <td>-log(0.9) ≈ 0.105</td>
      </tr>
    </tbody>
  </table>

  <h3>Applications:</h3>
  <ul>
    <li>Widely used in binary classification problems.</li>
    <li>Default loss function in many libraries such as <code>sklearn.metrics.log_loss</code> and <code>binary_crossentropy</code> in Keras.</li>
    <li>Especially useful for models that output probabilities, such as Logistic Regression or Neural Networks.</li>
  </ul>

  <p>In summary, <strong>Log Loss penalizes confident but incorrect predictions more heavily</strong>, making it a powerful and reliable metric for classification performance.</p>

