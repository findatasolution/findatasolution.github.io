<h1>Loss Functions of Popular Machine Learning Algorithms</h1>
<h2>Master table</h2>
  <table>
    <thead>
      <tr>
        <th></th> 
        <th>Algorithm</th>
        <th>Typical Use Case</th>
        <th>Common Loss Function(s)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="assets\images\LR.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Linear Regression</td>
        <td>Regression</td>
        <td>MSE, MAE, Huber Loss, <br>Quantile Loss</td>
      </tr>
      <tr>
        <td><img src="assets\images\LogR.png" style="width: 100px; height: 100px; object-fit: contain;"></td>
        <td>Logistic Regression</td>
        <td>Binary Classification</td>
        <td>Log Loss, <br>Hinge Loss (occasionally)</td>
      </tr>
      <tr>
        <td><img src="assets\images\DecisionTree.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Decision Tree:
          <br> 1. Ensemble methods
          <br> 1.1 Bagging - Random Forest
          <br> 1.2.1 Boosting - AdaBoost
          <br> 1.2.2 Boosting - Gradient Boosting
          <br> 1.2.2.1 Boosting - Gradient Boosting - CatBoost
          <br> 1.2.2.2 Boosting - Gradient Boosting - LightGBM
          <br> 1.2.2.3 Boosting - Gradient Boosting - XGBoost
        </td>
        <td>Classification <br> Regression</td>
        <td>Gini Impurity,Entropy (Information Gain) <br> MSE, MAE, Huber Loss </td>
      </tr>
      <tr>
        <td></td>
        <td>SVM</td>
        <td>Classification</td>
        <td>Hinge Loss, Squared Hinge Loss, Logistic Loss</td>
      </tr>
      <tr>
        <td></td>
        <td>k-NN</td>
        <td>Classification, <br> Regression</td>
        <td>No direct loss; uses distance metrics (e.g., Euclidean, Manhattan)</td>
      </tr>
      <tr>
        <td></td>
        <td>Naive Bayes</td>
        <td>Classification</td>
        <td>Negative Log-Likelihood, <br>no explicit loss function</td>
      </tr>
      <tr>
        <td></td>
        <td>Neural Networks</td>
        <td>Classification, <br>Regression</td>
        <td>Regression: MSE, MAE, Huber<br>Classification: Cross-Entropy, Focal Loss, KLDivergence</td>
      </tr>
      <tr>
        <td></td>
        <td>K-Means</td>
        <td>Unsupervised</td>
        <td>Within-Cluster Sum of Squares (WCSS)</td>
      </tr>
      <tr>
        <td></td>
        <td>Reinforcement Learning</td>
        <td>Decision Making</td>
        <td>TD Error, Policy Gradient Loss, Actor-Critic Loss</td>
      </tr>
    </tbody>
  </table>

  <div style="text-align: center;">
    <img src="/../assets/images/lossfunctions.png" alt="Common Loss Functions" style="max-width: 100%; height: auto; margin: 20px 0;">
    <p style="font-style: italic; color: #666;">Figure: Visual comparison of common loss functions showing their behavior and characteristics</p>
  </div>
  <h2>MSE - Mean Squared Error</h2>
  <p>MSE is a loss function that measures the average of the squares of the errors. It is a popular loss function for regression problems.</p>
  <p>The MSE function is given by:</p>
  <p>
      $$\operatorname{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left( Y_{i} - \hat{Y_{i}} \right)^{2}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>MSE is sensitive to outliers because it squares the errors.</p>
  
  <h2>MAE - Mean Absolute Error</h2>
  <p>MAE is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The MAE function is given by:</p>
  <p>
      $$\operatorname{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| Y_{i} - \hat{Y_{i}} \right|$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>MAE is less sensitive to outliers because it does not square the errors.</p>
  <h2>R square</h2>
  <p>R square is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The R square function is given by:</p>
  <p>
      $$R^2 = 1 - \frac{\sum_{i=1}^{n} \left( Y_{i} - \hat{Y_{i}} \right)^{2}}{\sum_{i=1}^{n} \left( Y_{i} - \bar{Y} \right)^{2}}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>R square is used to measure the goodness of fit of a regression model.</p>

  <h2>Log Loss - Binary Cross-Entropy</h2>
  <p>Log Loss is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The Log Loss function is given by:</p>
  <p>
      $$L(y, \hat{y}) = -\frac{1}{n} \sum_{i=1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>Log Loss is used in binary classification problems.</p>

  <h2>Huber Loss</h2>
  <p>Huber Loss is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The Huber Loss function is given by:</p>
  <p>   
      $$L_{\delta}(y, \hat{y}) = \begin{cases} 
      \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
      \delta(|y - \hat{y}| - \frac{\delta}{2}) & \text{if } |y - \hat{y}| > \delta 
      \end{cases}$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  <p>Huber Loss combines the best properties of MSE and MAE:</p>
  <ul>
    <li>For small errors (when |y - ŷ| ≤ δ), it behaves like MSE, using the squared error term</li>
    <li>For large errors (when |y - ŷ| > δ), it behaves like MAE, using the linear error term</li>
  </ul>

  <p>The parameter δ (delta) controls the transition point between the quadratic and linear portions:</p>
  <ul>
    <li>When δ → ∞, Huber Loss becomes equivalent to MSE</li>
    <li>When δ → 0, Huber Loss becomes equivalent to MAE</li>
  </ul>

  <p>Key advantages of Huber Loss:</p>
  <ul>
    <li>Combines the differentiability of MSE near zero with the robustness of MAE for outliers</li>
    <li>Provides stable gradients for small errors while being less sensitive to outliers</li>
    <li>The δ parameter can be tuned to optimize the trade-off between MSE and MAE behavior</li>
  </ul>

  <p>This makes Huber Loss particularly useful in scenarios where:</p>
  <ul>
    <li>The data contains outliers but you still want smooth gradients near the minimum</li>
    <li>You need a compromise between the high sensitivity of MSE and the constant-gradient nature of MAE</li>
    <li>The model needs to be robust while maintaining good convergence properties</li>
  </ul>

  <h2>Quantile Loss</h2>
  <p>Quantile Loss is a loss function that measures the average of the absolute errors. It is a popular loss function for regression problems.</p>
  <p>The Quantile Loss function is given by:</p>
  <p>
      $$L_{\tau}(y, \hat{y}) = (y - \hat{y})(\tau - I(y < \hat{y}))$$
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>  
  


  

