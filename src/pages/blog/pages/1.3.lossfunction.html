<h1>ðŸ“Š Loss Functions of Popular Machine Learning Algorithms</h1>
<h2>Master table</h2>
  <table>
    <thead>
      <tr>
        <th></th>
        <th>Algorithm</th>
        <th>Typical Use Case</th>
        <th>Common Loss Function(s)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="assets\images\LR.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Linear Regression</td>
        <td>Regression</td>
        <td>Mean Squared Error (MSE), <br>Mean Absolute Error (MAE), <br>Huber Loss, <br>Quantile Loss</td>
      </tr>
      <tr>
        <td><img src="assets\images\LogR.png" style="width: 100px; height: 100px; object-fit: contain;"></td>
        <td>Logistic Regression</td>
        <td>Binary Classification</td>
        <td>Binary Cross-Entropy (Log Loss), <br>Hinge Loss (occasionally)</td>
      </tr>
      <tr>
        <td><img src="assets\images\DecisionTree.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>
          Decision Tree Classifier:<br> - Random Forest Classifier <br> - Boosting :
          <br> -- AdaBoost, 
          <br> -- Gradient Boosting (LightGBM / CatBoost / XGBoost)</td>
        <td>Classification</td>
        <td>Gini Impurity, <br>Entropy (Information Gain)</td>
      </tr>
      <tr>
        <td><img src="assets\images\DecisionTree.png" style="width: 100px; height: 100px; object-fit: contain;"> </td>
        <td>Decision Tree Regressor:<br> - Random Forest Classifier <br> - Boosting :
          <br> -- AdaBoost, 
          <br> -- Gradient Boosting (LightGBM / CatBoost / XGBoost)</td>
        <td>Regression</td>
        <td>MSE, MAE, Huber Loss</td>
      </tr>
      <tr>
        <td></td>
        <td>AdaBoost</td>
        <td>Classification</td>
        <td>Exponential Loss, <br>Log Loss (LogitBoost)</td>
      </tr>
      <tr>
        <td></td>
        <td>Gradient Boosting</td>
        <td>Classification, <br>Regression</td>
        <td>Regression: MSE, MAE, Huber<br>Classification: Log Loss, Deviance, Cross-Entropy</td>
      </tr>
      <tr>
        <td></td>
        <td>SVM</td>
        <td>Classification</td>
        <td>Hinge Loss, Squared Hinge Loss, Logistic Loss</td>
      </tr>
      <tr>
        <td></td>
        <td>k-NN</td>
        <td>Classification, <br> Regression</td>
        <td>No direct loss; uses distance metrics (e.g., Euclidean, Manhattan)</td>
      </tr>
      <tr>
        <td></td>
        <td>Naive Bayes</td>
        <td>Classification</td>
        <td>Negative Log-Likelihood, <br>no explicit loss function</td>
      </tr>
      <tr>
        <td></td>
        <td>Neural Networks</td>
        <td>Classification, <br>Regression</td>
        <td>Regression: MSE, MAE, Huber<br>Classification: Cross-Entropy, Focal Loss, KLDivergence</td>
      </tr>
      <tr>
        <td></td>
        <td>K-Means</td>
        <td>Unsupervised</td>
        <td>Within-Cluster Sum of Squares (WCSS)</td>
      </tr>
      <tr>
        <td></td>
        <td>Reinforcement Learning</td>
        <td>Decision Making</td>
        <td>TD Error, Policy Gradient Loss, Actor-Critic Loss</td>
      </tr>
    </tbody>
  </table>

  <h2>MSE</h2>
  <p>MSE is a loss function that measures the average of the squares of the errors. It is a popular loss function for regression problems.</p>
  <p>MSE is defined as:</p>
  <p>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="math/tex">MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2</script>
  </p>
  <p>where $n$ is the number of samples, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value.</p>
  
